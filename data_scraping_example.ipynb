{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_scraping_example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "YdN3bu8GuGPm",
        "1bVVlFietRvT",
        "NeIagE1V-LCN",
        "Mhm18HbU-O-t",
        "318D_Kjk-Rcn",
        "ZqiXTW8heAAZ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jordanml7/DroneSimulation/blob/master/data_scraping_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQF8vNbq8RbN",
        "colab_type": "text"
      },
      "source": [
        "# Example Implementation of Website Scraping\n",
        "\n",
        "*Written by Jordan Lueck and Paddy Alton*\n",
        "\n",
        "**How to use:**\n",
        "\n",
        "First you have to run the installation & importing codeblocks, below. To run a codeblock, simple mouse of the little number in the top left of the codeblock and a small *play button* will appear - click it!\n",
        "\n",
        "Next, determine the form of your data. Is it a text-centered webpage? Or is it a dataset, like a big table? If it's some other type of data you're trying to scrape, come ask Jordan or Paddy!\n",
        "\n",
        "Pick the relevant subsection of this doc & uncollapse it to see your next steps...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdN3bu8GuGPm",
        "colab_type": "text"
      },
      "source": [
        "## Import necessary libraries\n",
        "\n",
        "You *always* need to run these blocks first before running any other part of this code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgeLiKAlgqsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install apolitical-data-viz -qU\n",
        "!pip install geopandas -qU\n",
        "!pip install bokeh -qU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBk1GGng8KtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import apol_dataviz\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "import geopandas as gpd\n",
        "import re\n",
        "\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.models import LogColorMapper, LogTicker, ColorBar\n",
        "import bokeh.palettes as palettes\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models.annotations import Title"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bVVlFietRvT",
        "colab_type": "text"
      },
      "source": [
        "## Scraping text\n",
        "\n",
        "This section is useful if you've got an article or some press release that you'd like to scrape for its raw contents - i.e. the words. It's similar to just copying-and-pasting the text, but will do it for you in one step and remove any unnecessary graphics and whitespace.\n",
        "\n",
        "The example executed below uses a Guardian report on the weather in the UK on Jan 31, 2019. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPzFH78PVajB",
        "colab_type": "text"
      },
      "source": [
        "Simply copy-and-paste the url of the article or release that you want to scrape into the **url** codeblock below, then individually *run* that codeblock and the three below it. Your output text is now stored `stripped_text`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wchCVboftqWl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "url = \"https://www.theguardian.com/uk-news/2019/jan/31/britain-coldest-night-winter-mercury-drops-minus-11\" #@param{type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yn6vYF-tvt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_whitespace(plain_text):\n",
        "  \"\"\" Reduces multiple newline markers to one \"\"\"\n",
        "  text_copy = plain_text.strip(\"\\n\")\n",
        "  while True:\n",
        "    parsed = text_copy.split(\"\\n\\n\")\n",
        "    if len(parsed) < 2:\n",
        "      break\n",
        "    text_copy = \"\\n\".join(parsed)\n",
        "  return text_copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLT9yonUtuOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bPuoD7Ityra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b73d1628-65f1-4074-e188-7a577babe26a"
      },
      "source": [
        "plain_text = \"\\n\".join([p.get_text() for p in soup.findAll(\"p\")])\n",
        "\n",
        "stripped_text = reduce_whitespace(plain_text)\n",
        "print(stripped_text)"
      ],
      "execution_count": 518,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Snow and ice alert as Braemar in Scotland is hit by lowest temperature in UK since 2012\n",
            "Steven Morris\n",
            "Thu 31 Jan 2019 07.43 EST\n",
            "First published on Thu 31 Jan 2019 03.16 EST\n",
            "The lowest temperature in the UK for seven years was recorded on Thursday as snowy and icy weather continued to hit Britain.\n",
            "Residents of Braemar in north-east Scotland were shivering in a temperature of -14.4C (6.1F), the Met Office said – the lowest temperature in the UK since 2012 when it reached -15.6C (3.9F) at Holbeach, Lincolnshire.\n",
            "UPDATE: Braemar has now fallen to -14.4 °C. That's the lowest temperature in the UK since 2012 (-15.6 °C at Holbeach, Lincolnshire 11 February) pic.twitter.com/f1PVbiwDIZ\n",
            "Meanwhile an amber severe weather warning was issued on Thursday by the Met Office for London, south-east and south-west England and Wales.\n",
            "An amber severe weather warning for #snow has been issued: https://t.co/QwDLMfRBfs Stay #weatheraware @metofficeuk pic.twitter.com/nCQoqsXgiC\n",
            "The Met Office said: “A band of rain will arrive from the southwest on Thursday afternoon, quickly turning to snow and becoming heavy at times.”\n",
            "It said 3cm to 7cm could accumulate within two to three hours and there could be up to 10cm in some places. “The highest snowfall accumulations are likely to be in areas above 150m,” the Met Office added.\n",
            "All four countries in the UK set new records for this winter, with Sennybridge in Powys, Wales, dropping to -9.3C (15.3F), Katesbridge in Northern Ireland falling to -8.2C (17.2F) and Redesdale Camp in Northumberland recording a temperature of -10.4C (13.3F).\n",
            "The Met Office issued weather warnings for snow, ice and fog on Thursday and into Friday. Mark Wilson, a Met Office forecaster, said the cold temperatures would stick around.\n",
            "“It’s been a very, very cold night,” he said. “On Thursday night we could see similar temperatures in Scotland, but Northern Ireland, England and Wales probably won’t be quite as cold. Saturday night into Sunday could also be very cold.”\n",
            "Southeastern trains said 21 services were being cancelled or altered on Thursday morning to minimise the impact of ice forming on the rails and ensure lines were clear. It said it would also run its winter weather timetable on Friday due to the forecast.\n",
            "Manchester and Liverpool airports were brought to a standstill on Wednesday morning and several schools across the UK were closed owing to the harsh conditions.\n",
            "Wilson said central England and Wales were likely to bear the brunt of the snow on Thursday afternoon and into the evening, with up to 10cm (4in) of snow in some places.\n",
            "Yellow weather warnings for ice were in place for north-west Scotland and the Northern and Western Isles, alerting travellers to be wary of slippery roads. Similar warnings were in place for north-west and south-west England, Yorkshire and the Humber and Wales.\n",
            "People in the Midlands, east of England, Greater London and the south of England were advised to beware of fog making driving on icy roads more difficult on Thursday morning.\n",
            "A yellow warning for snow was in place across swaths of England and was due to remain until late on Friday night. Most of Scotland should also be prepared for snowfall, the Met Office said.\n",
            "The Met Office warned heavy snow could cause:\n",
            "Travel delays on roads and lead to some vehicles becoming stranded;\n",
            "Delays and cancellations to rail travel;\n",
            "Rural communities being cut off;\n",
            "Power cuts and problems with mobile phone coverage.\n",
            "The snow has already led to the cancellation of some sporting events including race meetings at Fakenham in Norfolk and Wincanton in Somerset.\n",
            "In Wales, Dyfed-Powys police said it expected to be busy and asked for people to think twice before contacting the force. \n",
            "Head of specialist operations, Supt Craig Templeton, said: “We have been busy planning for the adverse weather so we can continue to serve our communities in the way they expect.\n",
            "“To help us do this, we are asking the public to consider their options before contacting us, as in some cases, other organisations may be better placed to help you.\n",
            "“For example, if you are calling about a tree or power/telephone pole that is down you should only call us to report this if there is a significant risk to others, or there has been a collision and someone is injured.”\n",
            "Sgt Olly Taylor, lead investigator for fatal road crashes in Devon and Cornwall, said: “Please think about whether your journey is really essential. Emergency services are likely to be busy.”\n",
            "The AA is advising motorists to carry a winter survival kit containing items such as an ice scraper, de-icer and a blanket.\n",
            "A spokesman said: “People should also take it slow, as stopping distances take 10 times longer. Gentle manoeuvres are the key to safe driving in ice and snow.”\n",
            "Bookmaker Coral has slashed the odds on this February being the coldest on record, now making it just 6-4.\n",
            "“We’ve had a freezing end to January and forecasts are suggesting that the next week or so will be more of the same. This means that February will begin with every chance of going into the record books as the coldest ever seen and we’ve slashed the odds on that coming true,” said Coral’s Harry Aitkenhead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mra2JRrfs_W4",
        "colab_type": "text"
      },
      "source": [
        "## Scraping a dataset\n",
        "\n",
        "If you're trying to scrape a dataset of some sort, first you've got to determine the form of the dataset. Is it a CSV you've downloaded? Or is a table you've found online somewhere? Again, if your data is in some other form, come as Jordan or Paddy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeIagE1V-LCN",
        "colab_type": "text"
      },
      "source": [
        "### If it's a CSV / TSV ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCn9JDyibxH0",
        "colab_type": "text"
      },
      "source": [
        "If you've downloaded the dataset to your own computer in a CSV or TSV format, upload the file into your Google Colab environment by opening the sidebar on the left of this window and selecting the **Files** section. Click **upload**.\n",
        "\n",
        "Next, enter the entire name of the file as it appears in the sidebar, including the filetype extension (i.e. `my_dataset.csv`), into the **dataset** field below. Now individually *run* that codeblock and the 4 blocks below it.\n",
        "\n",
        "Note that this code is currently configured to handle geographic data from `Eurostat` sources only. This can easily be adapted for other types of datasets and other sources as necessary. Additionally, the example below is extracting from a TSV file uploaded to Github, rather than a local file, so every user can access it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDdC9mzXtdR_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "dataset = \"https://raw.githubusercontent.com/apolitical/journalism/scraping-tutorial/prepared-data/t2020_30.tsv\" #@param{type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg_TcyvVi67c",
        "colab_type": "text"
      },
      "source": [
        "Since you are using a CSV/TSV, you'll have to supply the name of the dataset, i.e. what the dataset is measuring, manually below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCF1Ber3iww4",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "dataset_name = \"Greenhouse Gas Emissions\" #@param{type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eLHsupdXbpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if dataset.endswith(\".csv\"):\n",
        "  df = pd.read_csv(dataset)\n",
        "elif dataset.endswith(\".tsv\"):\n",
        "  df = pd.read_csv(dataset,\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbJ5DmpcYfXh",
        "colab_type": "text"
      },
      "source": [
        "All the below code really does is mess around with some formatting from the `Eurostat` dataset to make it compatible with the methods for extracting and plotting its data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQjmnDdiLAhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  ind_geo = df.columns[0].split(\"\\\\\")[0].split(\",\").index(\"geo\")\n",
        "except:\n",
        "  ind_geo = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "889WxBFgAPJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.rename(index=str, columns={df.columns[0]: \"iso_a2\"})\n",
        "\n",
        "temp = df[\"iso_a2\"].tolist()\n",
        "code = [n.split(\",\")[ind_geo] for n in temp]\n",
        "df[\"iso_a2\"] = code\n",
        "\n",
        "df.loc[df.iso_a2 == \"UK\",\"iso_a2\"] = \"GB\"\n",
        "df.loc[df.iso_a2 == \"EL\",\"iso_a2\"] = \"GR\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liANeKAiYsvO",
        "colab_type": "text"
      },
      "source": [
        "The below CSV contains the various codes used by the UN and other organizations to define nations (it's easier to compare a 3-digit code, like **826**, than a complicated name, like **The United Kingdom of Great Britain and Northern Ireland**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CErENlu5ftPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "country_codes = pd.read_csv(\"https://raw.githubusercontent.com/apolitical/journalism/scraping-tutorial/prepared-data/country_codes_complete.csv\")\n",
        "country_codes.loc[country_codes.iso_a3 == \"NAM\",\"iso_a2\"] = \"NA\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iszvQ-8R4fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_df = pd.merge(df, country_codes, how=\"left\", on=\"iso_a2\").drop(columns=[\"m49\",\"iso_a3\"])\n",
        "csv_df.loc[csv_df.country.isnull(),\"country\"] = csv_df.loc[csv_df.country.isnull(),\"iso_a2\"]\n",
        "\n",
        "csv_df.index = csv_df[\"country\"]\n",
        "csv_df = csv_df.drop(columns=[\"country\",\"iso_a2\"])\n",
        "csv_df.columns = [year.strip(' ') for year in csv_df.columns.tolist()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHX4zP9vJtjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_numeric(x):\n",
        "  return re.sub(\"[^0-9.]\", \"\", x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BztaRna0JwIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data_df = csv_df.applymap(format_numeric)\n",
        "final_data_df = final_data_df.replace(\":\", np.nan, regex=True).replace(\"\", np.nan, regex=True).astype(float).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhm18HbU-O-t",
        "colab_type": "text"
      },
      "source": [
        "### If it's a website (HTML) ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI3zlEMMZjPD",
        "colab_type": "text"
      },
      "source": [
        "If you've found the dataset online, it's probably in HTML format. Simply copy the link to the webpage that the table appears on and paste it into the **link_address** field below. Then, *run* the two codeblocks below.\n",
        "\n",
        "**NB**: While it's easier and quicker to load a webpage than download & upload a CSV, and it is accessible by everyone as opposed to just those with the relevant files, it can often be risky as different data sources will format their HTML tables differently, potentially leading to some funky results. Just something to keep in mind."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hCWUVd8ZVB0",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "link_address = \"https://ec.europa.eu/eurostat/tgm/table.do?tab=table&init=1&language=en&pcode=t2020_30&plugin=1\" #@param{type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLB9Qg86kLNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "html_df = pd.read_html(link_address)\n",
        "dataset_name = html_df[-5][0][0].title()\n",
        "final_data_df = html_df[-1]\n",
        "rows = html_df[-2].columns.tolist()[0]\n",
        "cols = html_df[-3].columns.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wec6mmXpofRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_numeric(x):\n",
        "  return re.sub(\"[^0-9.]\", \"\", x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wczIq0O9otOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data_df = final_data_df.applymap(format_numeric)\n",
        "final_data_df.columns = cols\n",
        "final_data_df.index = rows\n",
        "final_data_df = final_data_df.replace(\":\", np.nan, regex=True).replace(\"\", np.nan, regex=True).astype(float).sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCD35qC5bGn_",
        "colab_type": "text"
      },
      "source": [
        "The below block is commented out and currently does nothing. It's just here to show you some of the stuff that's going on \"behind-the-hood\" of the above `read_html` method called. This can be useful if you want to dig into the data extraction, perhaps if `read_html` isn't producing the data the way you think it should appear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60JFEz3TaiWi",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The longer way...\n",
        "(useful if you want to specify how exactly the data is extracted)\n",
        "\n",
        "response = requests.get(link_address)\n",
        "html_doc = response.text\n",
        "\n",
        "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
        "data_title = soup.h2.string\n",
        "\n",
        "html_columns = soup.find_all(\"th\", class_=\"cell\")\n",
        "column_headers = [h.text.strip() for h in html_columns]\n",
        "\n",
        "html_rows = soup.find_all(\"th\", class_=\"hl_row_fix\")\n",
        "row_headers = [r.text.strip() for r in html_rows]\n",
        "\n",
        "html_df = pd.DataFrame(index=row_headers, columns=column_headers)\n",
        "\n",
        "for row in html_rows:\n",
        "  header = row.text.strip()\n",
        "  id_name = row.get(\"id\").replace(\"_fix\",\"\")\n",
        "  \n",
        "  row_stats = soup.find(\"tr\", id=id_name).find_all(\"td\")\n",
        "  row_data = [float(r.text.strip().replace(\":\",\"NaN\")) for r in row_stats]\n",
        "  \n",
        "  html_df.loc[header] = row_data\n",
        "  \n",
        "html_df = html_df.sort_index()\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "318D_Kjk-Rcn",
        "colab_type": "text"
      },
      "source": [
        "### What to do with the data...\n",
        "\n",
        "Now you've got your dataset, what do you want to do with it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqiXTW8heAAZ",
        "colab_type": "text"
      },
      "source": [
        "#### You could present it as a line graph...\n",
        "\n",
        "Just select the country you want to plot the yearly data for and run all the following codeblocks!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UffHzE8Sih0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "countries = final_data_df.index.tolist()\n",
        "country_picker = widgets.Dropdown(options=countries, value=countries[0])\n",
        "country_picker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQofeCxBc7p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "country_choice = country_picker.value\n",
        "\n",
        "country_data = final_data_df.loc[country_choice]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ZMJ3AEjVZ8",
        "colab_type": "text"
      },
      "source": [
        "You will need to supply your desired x-axis and y-axis labels yourself, below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq80NtDYjf7Y",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "xlab = \"Year\" #@param{type: \"string\"}\n",
        "ylab = \"Percent (%) of 1990 Emissions\" #@param{type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO--3p1_bPNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = country_data.plot(title=country_choice + \" \" + dataset_name)\n",
        "xlab = ax.set_xlabel(xlab)\n",
        "ylab = ax.set_ylabel(ylab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsSFyb6ieC7N",
        "colab_type": "text"
      },
      "source": [
        "#### Or you could make an interactive world map...\n",
        "\n",
        "Just select the year you want to plot the geographic country data for and run all the following codeblocks!\n",
        "\n",
        "**NB**: This is a rather finnicky graph that its quite case-specific, so changing the data is likely to break the graph. If this happens, reach out to Jordan or Paddy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VrfVplBeILf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "years = final_data_df.columns.tolist()\n",
        "year_picker = widgets.Dropdown(options=years, value=years[len(years)-2])\n",
        "year_picker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MOGBy-deVy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "year_choice = year_picker.value\n",
        "\n",
        "year_data = final_data_df.loc[:,year_choice]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxG5hRwBIrs-",
        "colab_type": "text"
      },
      "source": [
        "##### Behind the scenes of map generation..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wJ6X26ufnAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ylr = palettes.YlOrRd6\n",
        "ylr.reverse()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip60slXrfpix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
        "world = world.drop(world.loc[world.continent == \"Antarctica\"].index.tolist())\n",
        "\n",
        "world.loc[world.name == \"France\",\"iso_a3\"] = \"FRA\"\n",
        "world.loc[world.name == \"Norway\",\"iso_a3\"] = \"NOR\"\n",
        "world.loc[world.name == \"N. Cyprus\",\"iso_a3\"] = \"CYP\"\n",
        "world.loc[world.name == \"Somaliland\",\"iso_a3\"] = \"SOM\"\n",
        "world.loc[world.name == \"Kosovo\",\"iso_a3\"] = \"RKS\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyiLM8ECg2az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "geo_data = year_data.to_frame().reset_index().rename(columns={\"index\": \"country\"})\n",
        "geo_data.loc[geo_data.country == \"United Kingdom\",\"country\"] = \"United Kingdom of Great Britain and Northern Ireland\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb7dAfXbfQOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "country_codes = pd.read_csv(\"https://raw.githubusercontent.com/apolitical/journalism/scraping-tutorial/prepared-data/country_codes_complete.csv\")\n",
        "country_codes.loc[country_codes.iso_a3 == \"NAM\",\"iso_a2\"] = \"NA\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whoIbjtliDTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expanded_world = pd.merge(country_codes, geo_data, how=\"left\", on=\"country\")\n",
        "complete_data = pd.merge(world, expanded_world, how=\"left\", on=\"iso_a3\").replace(np.nan, \"-\", regex=True)\n",
        "complete_data = complete_data.drop(columns=[\"pop_est\",\"continent\",\"name\",\"iso_a3\",\"gdp_md_est\",\"m49\"])\n",
        "complete_data.loc[complete_data.loc[:,year_choice] == \"-\",year_choice] = \"No Data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuRIJFCMjHgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newdf = pd.DataFrame({})\n",
        "for index, row in complete_data.iterrows():\n",
        "  if row.geometry.type == \"MultiPolygon\":\n",
        "    geom_data = row.geometry\n",
        "    for polygon in geom_data:\n",
        "      newrow = row.copy()\n",
        "      newrow.geometry = polygon\n",
        "      newdf = newdf.append(newrow, ignore_index=True)\n",
        "    complete_data.drop(index, axis=0, inplace=True)\n",
        "\n",
        "complete_data = complete_data.append(newdf, sort=True, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxCPqaN3jItl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "complete_data[\"x\"] = [list(poly.exterior.coords.xy[0]) for poly in complete_data.geometry]\n",
        "complete_data[\"y\"] = [list(poly.exterior.coords.xy[1]) for poly in complete_data.geometry]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7jg5leSjLVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = \"http://flagpedia.net/data/flags/w580/\"\n",
        "\n",
        "good_data = complete_data[complete_data.loc[:,year_choice] != \"No Data\"]\n",
        "nan_data = complete_data[complete_data.loc[:,year_choice] == \"No Data\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA_NuOPOpPcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "low_val = int(min(good_data.loc[:,year_choice]))\n",
        "hi_val = int(max(good_data.loc[:,year_choice]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbT0ADsqjRbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "color_mapper = LogColorMapper(palette=ylr, low=low_val, high=hi_val)\n",
        "\n",
        "good = dict(\n",
        "    x=good_data.x.tolist(),\n",
        "    y=good_data.y.tolist(),\n",
        "    name=good_data.country.tolist(),\n",
        "    data=[float(i) for i in good_data.loc[:,year_choice]],\n",
        "    img=[a + i.lower() + \".png\" for i in good_data.iso_a2],\n",
        ")\n",
        "\n",
        "nan = dict(\n",
        "    x=nan_data.x.tolist(),\n",
        "    y=nan_data.y.tolist(),\n",
        "    name=nan_data.country.tolist(),\n",
        "    data=nan_data.loc[:,year_choice].tolist(),\n",
        "    img=[a + i.lower() + \".png\" for i in nan_data.iso_a2],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPasLeoxjhhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOOLS = \"pan,box_zoom,wheel_zoom,reset,hover,save\"\n",
        "title_string = dataset_name + \", \" + year_choice\n",
        "data_name = title_string\n",
        "if len(data_name) > 40:\n",
        "  list_words = title_string.split(' ')\n",
        "  list_words[round(len(list_words)/2)] = list_words[round(len(list_words)/2)] + \"<br>\"\n",
        "  data_name = ' '.join(list_words)\n",
        "\n",
        "tooltips = \"\"\"\n",
        "<div style=\"width:300px;padding: 5px;border-style: solid;border-width: 1px;border-color: #00B3BF;\">\n",
        "  <div>\n",
        "    <img\n",
        "        src=\"@img\" height=\"30\" width=\"50\"\n",
        "        style=\"float: left; margin: 0px 10px 0px 0px;\"\n",
        "        border=\"1\"\n",
        "    ></img>\n",
        "  </div>\n",
        "  <div>\n",
        "    <div style=\"height:30px\">\n",
        "      <b><font size=2vh; color=#00B3BF>@name</font></b>\n",
        "    </div>\n",
        "    <div style=\"padding-top:10px\">\n",
        "      <b>{data_name}:</b> @data <br>\n",
        "    </div>\n",
        "    <div>\n",
        "      <img\n",
        "          src=\"https://apolitical.co/wp-content/themes/apolitical/public/img/stamp.svg\" height=\"16\" width=\"12\"\n",
        "          style=\"position: absolute; bottom: 10px; right: 12px;\"\n",
        "      ></img>\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\".format(data_name=data_name,link=more_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUMA3YGPkBu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title = Title(text=title_string, text_color=\"#00B3BF\",text_font_size=\"20px\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2UbOtRNkFzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = figure(\n",
        "    title=title, tools=TOOLS,\n",
        "    x_axis_location=None, y_axis_location=None,\n",
        "    tooltips=tooltips,\n",
        "    plot_width=1000, plot_height=500, toolbar_location=\"below\",\n",
        "    active_scroll = \"wheel_zoom\", outline_line_color = \"#00B3BF\",\n",
        "    outline_line_width = 2)\n",
        "\n",
        "p.toolbar.logo = None\n",
        "p.grid.grid_line_color = None\n",
        "p.hover.point_policy = \"follow_mouse\"\n",
        "\n",
        "p.patches(\"x\", \"y\", source=good,\n",
        "          fill_color={\"field\": \"data\", \"transform\": color_mapper},\n",
        "          fill_alpha=0.7, line_color=\"white\", line_width=0.5)\n",
        "\n",
        "p.patches(\"x\", \"y\", source=nan, fill_color=\"#fbf7f5\",\n",
        "          fill_alpha=0.7, line_color=\"white\", line_width=0.5, hatch_pattern=\"/\", hatch_alpha=0.3)\n",
        "\n",
        "color_bar = ColorBar(color_mapper=color_mapper, ticker=LogTicker(),\n",
        "                     label_standoff=12, border_line_color=None, location=(0,0))\n",
        "\n",
        "p.add_layout(color_bar, \"right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irULJ38zIwOt",
        "colab_type": "text"
      },
      "source": [
        "##### ... And the final product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dij2z950kNGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_file(dataset_name.lower().split(',')[0].replace(' ','_') + \".html\")\n",
        "output_notebook()\n",
        "show(p)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}